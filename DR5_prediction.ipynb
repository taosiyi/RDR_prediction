{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edffb05",
   "metadata": {},
   "source": [
    "# DR5 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b684a14",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Train files: 7, Val files: 2\n",
      "‚úÖ Loaded 71 samples from 7 files (parallel 4 workers)\n",
      "‚úÖ Computed global mean/std for lumped & point\n",
      "‚úÖ Loaded 53 samples from 2 files (parallel 4 workers)\n",
      "time (data): 00:00:05.52\n",
      "train_dataset length: 71\n",
      "val_dataset length: 53\n",
      "Label shape: ()\n",
      "lumped_size:  17\n",
      "point_size:  196\n"
     ]
    }
   ],
   "source": [
    "from utils import make_train_val_datasets, BatteryRDRFullDataset, atteryRDRFullDataset_SF, format_time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "car_type = \"P\"\n",
    "folder_dir = rf\"data\\{car_type}\\train\"\n",
    "\n",
    "start_data = time.time()\n",
    "train_dataset, val_dataset, global_stats = make_train_val_datasets(\n",
    "    folder_dir,\n",
    "    val_ratio=0.2,\n",
    "    window_size=30,\n",
    "    step_size=10,\n",
    "    normalize=True,\n",
    "    dataset_class=BatteryRDRFullDataset,    # or BatteryRDRFullDataset_SF for selected features\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "data_time = time.time() - start_data\n",
    "print(f\"time (data): {format_time(data_time)}\")\n",
    "\n",
    "np.savez(\n",
    "    f\"global_stats.npz\",\n",
    "    lumped_mean=global_stats['lumped_mean'],\n",
    "    lumped_std=global_stats['lumped_std'],\n",
    "    point_mean=global_stats['point_mean'],\n",
    "    point_std=global_stats['point_std']\n",
    ")\n",
    "\n",
    "print('train_dataset length:', len(train_dataset))\n",
    "print('val_dataset length:', len(val_dataset))\n",
    "print('Label shape:', train_dataset[0]['label_future'].shape)\n",
    "\n",
    "lumped_size = train_dataset[0]['lumped'].shape[1]\n",
    "point_size = train_dataset[0]['point'].shape[0]\n",
    "print('lumped_size: ', lumped_size)\n",
    "print('point_size: ', point_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db412061",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MidFusionMMR_Transformer, train_multimodal_regressor\n",
    "import torch\n",
    "import os\n",
    "\n",
    "save_dir = 'model_MFT256'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "start_time = time.time()\n",
    "model = MidFusionMMR_Transformer(\n",
    "    lumped_embedding_size=lumped_size,\n",
    "    point_embedding_size=point_size,\n",
    "    hidden_size=256,\n",
    ")\n",
    "start_infer = time.time()\n",
    "\n",
    "train_losses, val_losses = train_multimodal_regressor(\n",
    "    model, save_dir,\n",
    "    train_dataset, val_dataset,\n",
    "    batch_size=64, epochs=100, lr=1e-3, \n",
    "    device='cuda', patience=20, min_lr=1e-5,\n",
    "    grad_clip=1.0, warmup_epochs=5\n",
    ")\n",
    "infer_time = time.time() - start_time\n",
    "print(f\"time (training): {format_time(infer_time)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581264ad",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd119bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import make_test_datasets_car_folder, BatteryRDRFullDataset, BatteryRDRFullDataset_SF, test_seg\n",
    "\n",
    "save_dir = \"test_seg_Fusion_SM\"\n",
    "car_type = \"P\"\n",
    "file_path = rf\"data\\{car_type}\\P\\test\"\n",
    "\n",
    "stats = np.load(f'global_stats_SF.npz')\n",
    "global_stats = {\n",
    "    'lumped_mean': stats['lumped_mean'],\n",
    "    'lumped_std': stats['lumped_std'],\n",
    "    'point_mean': stats['point_mean'],\n",
    "    'point_std': stats['point_std']\n",
    "}\n",
    "\n",
    "start_data = time.time()\n",
    "test_datasets = make_test_datasets_car_folder(\n",
    "    folder_dir, global_stats, car_type,\n",
    "    window_size=30, step_size=10, normalize=True, \n",
    "    dataset_class=BatteryRDRFullDataset,  # or BatteryRDRFullDataset_SF for selected features \n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(test_datasets, batch_size=1, shuffle=False)\n",
    "data_time = time.time() - start_data\n",
    "print(f\"time (data): {data_time:.4f}s\")\n",
    "\n",
    "\n",
    "model_name = 'MFT256'\n",
    "model = MidFusionMMR_Transformer(\n",
    "    lumped_embedding_size=15,\n",
    "    point_embedding_size=26,\n",
    "    hidden_size=256\n",
    ")\n",
    "start_data = time.time()\n",
    "test_seg(model_name, model, test_loader, save_dir)\n",
    "end_time = time.time() - start_data\n",
    "print(f\"time (infer): {end_time:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173ede6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a5132a2",
   "metadata": {},
   "source": [
    "## Tansfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51850bb3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Train files: 4192, Val files: 1049\n",
      "‚úÖ Loaded 130329 samples from 4192 files (parallel 4 workers)\n",
      "‚úÖ Computed global mean/std for lumped & point\n",
      "‚úÖ Loaded 33395 samples from 1049 files (parallel 4 workers)\n",
      "Êï∞ÊçÆËé∑ÂèñÊó∂Èó¥Ôºö00:01:46.20\n",
      "Êï∞ÊçÆËé∑ÂèñÊó∂Èó¥Ôºö106.1976s\n",
      "train_dataset length: 130329\n",
      "val_dataset length: 33395\n",
      "Label shape: ()\n",
      "lumped_size:  15\n",
      "point_size:  26\n"
     ]
    }
   ],
   "source": [
    "from utils import make_train_val_datasets, BatteryRDRFullDataset_SF, format_time\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "car_type = \"Q\"      # or \"R\"\n",
    "folder_dir = rf\"data\\{car_type}\\train\"\n",
    "\n",
    "start_data = time.time()\n",
    "train_dataset, val_dataset, global_stats = make_train_val_datasets(\n",
    "    folder_dir,\n",
    "    car_type,\n",
    "    val_ratio=0.2,\n",
    "    window_size=30,\n",
    "    step_size=10,\n",
    "    normalize=True,\n",
    "    dataset_class=BatteryRDRFullDataset_SF,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "data_time = time.time() - start_data\n",
    "print(f\"time (data): {format_time(data_time)}\")\n",
    "\n",
    "np.savez(\n",
    "    f\"global_stats_SF_{car_type}.npz\",\n",
    "    lumped_mean=global_stats['lumped_mean'],\n",
    "    lumped_std=global_stats['lumped_std'],\n",
    "    point_mean=global_stats['point_mean'],\n",
    "    point_std=global_stats['point_std']\n",
    ")\n",
    "\n",
    "print('train_dataset length:', len(train_dataset))\n",
    "print('val_dataset length:', len(val_dataset))\n",
    "print('Label shape:', train_dataset[0]['label_future'].shape)\n",
    "\n",
    "lumped_size = train_dataset[0]['lumped'].shape[1]\n",
    "point_size = train_dataset[0]['point'].shape[0]\n",
    "print('lumped_size: ', lumped_size)\n",
    "print('point_size: ', point_size)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394672fa",
   "metadata": {},
   "source": [
    "### MFT256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Module groups for MFT model =================\n",
    "TEMPORAL_ENCODER = [\n",
    "    \"rnn\",\n",
    "    \"norm_rnn\",\n",
    "]\n",
    "\n",
    "POINT_ENCODER = [\n",
    "    \"point_enc\",\n",
    "]\n",
    "\n",
    "CROSS_ATTENTION = [\n",
    "    \"cross_attn\",\n",
    "    \"norm_attn\", \n",
    "]\n",
    "\n",
    "MID_FUSION = [\n",
    "    \"fuse_gate\",\n",
    "    \"norm_fused\",\n",
    "]\n",
    "\n",
    "TRANSFORMER = [\n",
    "    \"transformer\",\n",
    "]\n",
    "\n",
    "HEAD = [\n",
    "    \"output\",\n",
    "]\n",
    "# ================= Fine-tuning strategies for MFT model =================\n",
    "FINE_TUNE_STRATEGIES_MFT = {\n",
    "    # Strategy 1: Linear Probing\n",
    "    \"LP\": [TEMPORAL_ENCODER + POINT_ENCODER + CROSS_ATTENTION + MID_FUSION + TRANSFORMER],\n",
    "    # Strategy 2: Fusion-level fine-tuning\n",
    "    \"CM\": [TEMPORAL_ENCODER + POINT_ENCODER + TRANSFORMER],\n",
    "    # Strategy 3: Non-encoder fine-tuning\n",
    "    \"NE\": [TEMPORAL_ENCODER + POINT_ENCODER],\n",
    "    # Strategy 4: Full fine-tuning\n",
    "    \"FF\": [[]],\n",
    "    # Strategy 5: Progressive fine-tuning\n",
    "    \"PG\": [\n",
    "        # Stage 1: heads only\n",
    "        TEMPORAL_ENCODER + POINT_ENCODER + CROSS_ATTENTION + MID_FUSION + TRANSFORMER,\n",
    "        # Stage 2: + mid-fusion & cross-attention\n",
    "        TEMPORAL_ENCODER + POINT_ENCODER + TRANSFORMER,\n",
    "        # Stage 3: + transformer\n",
    "        TEMPORAL_ENCODER + POINT_ENCODER,\n",
    "        # Stage 4: full\n",
    "        [],\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fdf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fine_tune,  MidFusionMMR_Transformer\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for strategy_name, freeze_plan in FINE_TUNE_STRATEGIES_MFT.items():\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"üöÄ Fine-tuning Strategy: {strategy_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    save_root = rf\"model_TL_MFT256_{car_type}/{strategy_name}\"\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "    model = MidFusionMMR_Transformer(\n",
    "        lumped_embedding_size=lumped_size,\n",
    "        point_embedding_size=point_size,\n",
    "        hidden_size=256,\n",
    "    ).to(device)\n",
    "\n",
    "    pretrained_path = \"model_MFT256/final_model.pth\"\n",
    "\n",
    "    # ---------- Single Strategy ----------\n",
    "    if len(freeze_plan) == 1:\n",
    "\n",
    "        print(f\"---- Strategy {strategy_name} | Single-stage ----\")\n",
    "        print(f\"Frozen modules: {freeze_plan[0]}\")\n",
    "\n",
    "        fine_tune(\n",
    "            pretrained_path=pretrained_path,\n",
    "            save_dir=save_root,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            model=model,\n",
    "            freeze_layers_list=freeze_plan[0],\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    # ---------- PG ----------\n",
    "    else:\n",
    "        prev_ckpt = pretrained_path\n",
    "\n",
    "        for stage_idx, freeze_list in enumerate(freeze_plan):\n",
    "\n",
    "            print(f\"\\n---- Strategy {strategy_name} | Stage {stage_idx + 1} ----\")\n",
    "            print(f\"Frozen modules: {freeze_list}\")\n",
    "\n",
    "            stage_dir = os.path.join(save_root, f\"S{stage_idx + 1}\")\n",
    "            os.makedirs(stage_dir, exist_ok=True)\n",
    "\n",
    "            fine_tune(\n",
    "                pretrained_path=prev_ckpt,\n",
    "                save_dir=stage_dir,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                model=model,\n",
    "                freeze_layers_list=freeze_list,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            prev_ckpt = os.path.join(stage_dir, \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05195393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b975781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
